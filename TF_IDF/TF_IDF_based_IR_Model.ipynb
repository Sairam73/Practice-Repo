{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e1acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from stemming.porter2 import stem\n",
    "import glob\n",
    "import string\n",
    "import math\n",
    " \n",
    "class Rcv1Doc:\n",
    "    def __init__(self, docID): #initialization\n",
    "        self.docID = docID\n",
    "        self.terms = {}\n",
    "        self.doc_len = 0\n",
    "    def get_doc_id(self):\n",
    "        return self.docID\n",
    "    def add_term(self, term):\n",
    "        stemmed_term = self.stem_term(term)\n",
    "        if stemmed_term not in self.terms:\n",
    "            self.terms[stemmed_term] = 1\n",
    "        else:\n",
    "            self.terms[stemmed_term] += 1\n",
    "        self.doc_len += 1\n",
    "    def stem_term(self, term):\n",
    "        return stem(term)\n",
    "\n",
    "def parse_rcv1v2(stop_words, inputpath):\n",
    "    collection = {}\n",
    "    os.chdir(inputpath)\n",
    "    for file_ in glob.glob(\"*.xml\"):  #iterates through all xml files in the path\n",
    "        docID = None\n",
    "        text = \"\"\n",
    "        start_end = False\n",
    "        for line in open(file_):        #reads each line of xml file \n",
    "            line = line.strip()\n",
    "            if not start_end:\n",
    "                if line.startswith(\"<newsitem \"):\n",
    "                    for part in line.split():\n",
    "                        if part.startswith(\"itemid=\"):\n",
    "                            docID = part.split(\"=\")[1].split(\"\\\"\")[1]       #gets docID\n",
    "                            break  \n",
    "                if line.startswith(\"<text>\"):\n",
    "                    start_end = True  \n",
    "            elif line.startswith(\"</text>\"):\n",
    "                break\n",
    "            else:\n",
    "                line = line.replace(\"<p>\", \"\").replace(\"</p>\", \"\")\n",
    "                line = line.translate(str.maketrans('', '', string.digits)).translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "                for term in line.split():\n",
    "                    term = stem(term.lower())            #applying stemming and converting to lower case\n",
    "                    if len(term) > 2 and term not in stop_words:\n",
    "                        if docID:\n",
    "                            if docID not in collection:\n",
    "                                doc = Rcv1Doc(docID)\n",
    "                                collection[docID] = doc\n",
    "                            collection[docID].add_term(term)\n",
    "                            text += term + \" \"\n",
    " \n",
    "        if docID:\n",
    "            print(\"Document ID:\", docID)\n",
    "            print(\"Term Frequencies:\", collection[docID].terms)\n",
    " \n",
    "    return collection\n",
    " \n",
    "def parse_query(query0, stop_words):\n",
    "    \n",
    "    query_terms = {} # Initialize a dictionary to store term frequencies\n",
    "    words = query0.translate(str.maketrans('', '', string.punctuation)).split()    # Remove punctuation characters from the query text and tokenize it\n",
    "    for word in words:    # For loop to process each word from the query\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and word.isalpha() and word.strip(): # Checks if the word is not a stop word, is alphabetic, and is not an empty string\n",
    "            stemmed_word = stem(word)\n",
    "            if stemmed_word in query_terms:\n",
    "                query_terms[stemmed_word] += 1\n",
    "            else:\n",
    "                query_terms[stemmed_word] = 1\n",
    "    return query_terms\n",
    " \n",
    "def read_stop_words(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        stop_words = file.read().split(',')\n",
    "    return stop_words\n",
    "def my_df(coll):\n",
    "    df = {}\n",
    "    for doc in coll:\n",
    "        terms = set(doc.keys())\n",
    "        for term in terms:\n",
    "            if term in df:\n",
    "                df[term] += 1\n",
    "            else:\n",
    "                df[term] = 1\n",
    "    return df\n",
    " \n",
    "def my_tfidf(doc, df, ndocs):\n",
    "    tfidf_weights = {}\n",
    "    sum_of_squares = 0.0\n",
    "    for term, freq in doc.items():\n",
    "        fik = freq  # Term frequency in the document\n",
    "        nk = df.get(term, 0)  # Document frequency of the term across all documents\n",
    "        log_term = (fik + 1) * (ndocs / nk) if nk != 0 else 0\n",
    "        tfidf_weights[term] = log_term\n",
    "        sum_of_squares += log_term ** 2\n",
    " \n",
    "    # Normalize TF*IDF weights\n",
    "    if sum_of_squares > 0:\n",
    "        normalization_factor = sum_of_squares ** 0.5\n",
    "        for term in tfidf_weights:\n",
    "            tfidf_weights[term] /= normalization_factor\n",
    " \n",
    "    return tfidf_weights\n",
    " \n",
    "def main():\n",
    "    \n",
    "    inputpath = r'C:\\Users\\0703s\\Downloads'\n",
    "    output_file = \"TF_IDF.txt\" \n",
    " \n",
    "    with open('common-english-words.txt', 'r') as stopwords_f:          # Load stop words\n",
    "        stop_words = set(stopwords_f.read().split(','))\n",
    " \n",
    "    Index = {}  # Initialize the index\n",
    "    os.chdir(inputpath)\n",
    "    for file_ in glob.glob(\"*.xml\"):\n",
    "        doc_terms = {}  # Initialize doc_terms for each document\n",
    "        docid = None  # Initialize docid to None        \n",
    "        start_end = False        #Flagging\n",
    "        with open(file_) as xml_file:\n",
    "            for line in xml_file:             # Iterate through the lines in the file\n",
    "                line = line.strip()\n",
    "                if not start_end:                  \n",
    "                    if line.startswith(\"<newsitem \"):                        \n",
    "                        for part in line.split():    # Iterate through the parts of the line\n",
    "                            if part.startswith(\"itemid=\"):        # Check if the part contains the itemid attribute                                \n",
    "                                docid = part.split(\"=\")[1].split(\"\\\"\")[1]\n",
    "                                break\n",
    "                    if line.startswith(\"<text>\"):          # Checks if the line contains the start of the text content\n",
    "                        start_end = True\n",
    "                elif line.startswith(\"</text>\"):\n",
    "                    break\n",
    "               \n",
    "                if start_end:               # If start tag has been encountered and end tag hasn't, process the line\n",
    "                    line = line.replace(\"<p>\", \"\").replace(\"</p>\", \"\")\n",
    "                    line = line.translate(str.maketrans('', '', string.digits)).translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "                    for term in line.split():\n",
    "                        term = stem(term.lower())         # Tokenize and process terms in the text content\n",
    "                        if len(term) > 2 and term not in stop_words:\n",
    "                            doc_terms[term] = doc_terms.get(term, 0) + 1\n",
    " \n",
    "        # Add doc_terms to Index with the corresponding docid\n",
    "        if docid is not None:\n",
    "            Index[docid] = doc_terms\n",
    " \n",
    " \n",
    "    # Calculate document frequency (df) for the collection\n",
    "    df = my_df(Index.values())  \n",
    "    with open(output_file, 'w') as f:           # Write output to the text file\n",
    "        f.write(f\"There are {len(Index)} documents in this data set and contain {len(df)} terms.\\n\")\n",
    "        f.write(\"The following are the termsâ€™ document-frequency:\\n\")\n",
    "        sorted_terms_df = sorted(df.items(), key=lambda x: x[1], reverse=True)\n",
    "        for term, freq in sorted_terms_df:\n",
    "            f.write(f\"{term} : {freq}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        for docid, doc in Index.items():\n",
    "            if len(doc) > 20:\n",
    "                tfidf_weights = my_tfidf(doc, df, len(Index))          #Calculate TF-IDF weights\n",
    "                sorted_terms_tfidf = sorted(tfidf_weights.items(), key=lambda x: x[1], reverse=True)[:20]   #Sort terms by TF-IDF weights in descending order\n",
    "                f.write(f\"Document {docid} contains {len(doc)} terms\\n\")\n",
    "                for term, weight in sorted_terms_tfidf:\n",
    "                    f.write(f\"{term}: {weight}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "    with open(output_file, 'a') as f:\n",
    "        query_titles = [\"ISRAEL: 15 Palestinians, two Israelis killed in clashes.\",\n",
    "                        \"CANADA: Great-West Life tops Royal Bank bid for London Ins.\",\n",
    "                        \"UK: Britain's Channel 5 to broadcast Fashion Awards.\"]  \n",
    "        for title in query_titles:\n",
    "            query = parse_query(title, stop_words)  # Ensure to pass stop_words\n",
    "            ranking_result = {}\n",
    "            for docid, doc in Index.items():\n",
    "                tfidf_weights = my_tfidf(doc, df, len(Index))\n",
    "                score = sum(tfidf_weights.get(term, 0) * query.get(term, 0) for term in query)\n",
    "                ranking_result[docid] = score\n",
    "            sorted_docs = sorted(ranking_result.items(), key=lambda x: x[1], reverse=True)       # Sort documents based on ranking score\n",
    "            f.write(f\"The Ranking Result for query: {title}\\n\")\n",
    "            for docid, score in sorted_docs:\n",
    "                f.write(f\"{docid} : {score}\\n\")\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
